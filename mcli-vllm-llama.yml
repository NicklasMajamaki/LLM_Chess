name: chess-reasoner-llama3_2-3b-instruct
image: lucasdino/llm_chess
compute:
    gpus: 8
    cluster: r7z23p1
scheduling:
  priority: lowest
  resumable: false
  preemptible: false
integrations:
  - integration_type: git_repo
    git_repo: NicklasMajamaki/LLM_Chess
command: |-
  # Activate the conda env
  source /opt/conda/etc/profile.d/conda.sh && conda activate llm_chess

  # URL to check
  URL="http://0.0.0.0:8000/v1/completions"

  # Time to wait between checks (in seconds)
  WAIT_TIME=10

  echo "Waiting for ${URL} to become available..."

  # Loop until the URL is available
  sleep 60
  until curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'> /dev/null; do       echo -n ".";       sleep "$WAIT_TIME";   done



  # Pull from git -- make sure we always just use the latest version online
  sleep 5
  git reset --hard HEAD
  git pull origin main
  sleep 20

  echo "Service is live. Starting the job..."

  # Run out main inference function
  python3 inference.py --model meta-llama/Llama-3.2-3B-Instruct \
  --base_url http://localhost:8000/v1 --batch_size 4 --max_evals 20 \
  --data_dir ./data
  # tmux attach -t 0
  # python3 hello_world.py --data_dir ./data

  echo "Finished the job."

dependent_deployment:
  image: vllm/vllm-openai:latest
  model: {}
  command: |-
    echo 'A bash command that downloads a model and then launches the server'
    set -x
    vllm serve "meta-llama/Llama-3.2-3B-Instruct" --tensor-parallel-size 8 \
    --trust-remote-code